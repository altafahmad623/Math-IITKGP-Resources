{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Data Mining (Autumn 2021)\n",
    "## Assignment 2 \n",
    "Name : <b>Varun Gupta</b> <br>\n",
    "Roll Number : <b>18MA20050</b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "<b>Task: </b> The aim of this assignment is to train and test Convolutional Nueral Networks for image classification on CIFAR10 dataset using PyTorch Module and to acquaint yourself with wandb which is a tool for monitoring large experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries\n",
    "Here, I have just used the already given code for importing the libraries, and also added the code for the extra libraries that I have used for developing the model, or for other required functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df17bac6e967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## all the torch libraries for Deep Learning Computation, and required functions, like loss and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "## all the torch libraries for Deep Learning Computation, and required functions, like loss and optimizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "## torchvision provides with the saved dataset and models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## required for other computation tasks for the project\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "## importing the required wandb library for logging all the required information\n",
    "import wandb\n",
    "\n",
    "## this just prevents wandb from printing its information, to decrease the clutter in the output \n",
    "%env WANDB_SILENT=true\n",
    "\n",
    "## checking if cuda is available, for faster computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## global var saves the best loss for each one of the models\n",
    "best_loss = float('inf')\n",
    "\n",
    "## logging into wandb account\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "- train_image: Consist of 50000 images of 32 x 32 RGB images\n",
    "- train_labels: Consist of 50000 labels from 10 classes for the images in train_images. The labels are described below.\n",
    "- test_images: Consist of 10000 images of 32 x 32 RGB images.\n",
    "- test_labels: Consist of 10000 labels from 10 classes for the images in test_images.\n",
    "<br>\n",
    "<b>Labels: </b>Each training and test image is classified into <b>ANY ONE</b> of the following labels:<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Labels | Labels\n",
    "| :- | :-|\n",
    "| 0 - Airplane | 1 - Automobile\n",
    "| 2 - Bird | 3 - Cat\n",
    "| 4 - Deer | 5 - Dog\n",
    "| 6 - Frog | 7 - Horse\n",
    "| 8 - Ship | 9 - Truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Load:\n",
    "Use the file <b>main.py</b> given in the [link](https://github.com/SoumiDas/CS60021_A2021) to load the data and carry on further experiments.The code here just downloads the data from the torchvision i.e. both the test and train data, and hence transforms the data as required so that it could be fed into the resnet18 model.<br>\n",
    "The Classes tuple contains the name of the classes according to their index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation\n"
     ]
    }
   ],
   "source": [
    "# Data Load: \n",
    "# Just used the main.py code given to download and transform the CIFAR10 dataset as required\n",
    "# for training the ResNet18 Model\n",
    "print('Data transformation')\n",
    "\n",
    "## tranformation for training set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "## tranformation for test set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "## downloading the training set\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=False, transform=transform_train)\n",
    "\n",
    "## using the dataloader so that we could iterated through minibatches of data while training \n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "## downloading the test set\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False, transform=transform_test)\n",
    "\n",
    "## similarly defining the testloader\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "## class names\n",
    "classes = ('Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "           'Dog', 'Frog', 'Horse', 'Ship', 'Truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)Model:\n",
    "Use the pretrained Resnet18 model to train your Convolutional Neural Network in the following ways.<br>\n",
    "a) <b>Train all the layers.</b><br>\n",
    "b) <b>Freeze the other layers and finetune only the last layer.</b><br><br>\n",
    "Here, I just initialize the four models required for devolepment according to the question, and for two of the models we just freeze the parameters in all their layers, and for the all the four models, we replace the final FC layer such that it outputs 10 classes and not 1000, also replacing in this manner, the last layer's parameters would automatically required to be trained.Finally just transfer all the models to the defined device so that if CUDA is available, faster computation would take place.<br><br> \n",
    "<b>Question: </b>You may get differences in accuracy in the above two methods. Describe what is the reason for the same.<br>\n",
    "<b>Solution: </b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model creation\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('Model creation')\n",
    "\n",
    "# defining all the required models \n",
    "net1_SGD = torchvision.models.resnet18(pretrained = True)\n",
    "net1_Adam = torchvision.models.resnet18(pretrained = True)\n",
    "net2_SGD = torchvision.models.resnet18(pretrained = True)\n",
    "net2_Adam = torchvision.models.resnet18(pretrained = True)\n",
    "\n",
    "# now, for the two of the models, freezing all the layers\n",
    "for param in net2_SGD.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in net2_Adam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "## number of input features in the last FC layer\n",
    "num_ftrs = net1_SGD.fc.in_features\n",
    "\n",
    "## for all models replacing the last layers number of output classes\n",
    "net1_SGD.fc = nn.Linear(num_ftrs,10)\n",
    "net1_Adam.fc = nn.Linear(num_ftrs,10)\n",
    "net2_SGD.fc = nn.Linear(num_ftrs,10)\n",
    "net2_Adam.fc = nn.Linear(num_ftrs,10)\n",
    "\n",
    "# using the models in the respective device, gives advantage if CUDA is available\n",
    "net1_SGD = net1_SGD.to(device)\n",
    "net1_Adam = net1_Adam.to(device)\n",
    "net2_SGD = net2_SGD.to(device)\n",
    "net2_Adam = net2_Adam.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)Training Module:\n",
    "Implement a mini-batch SGD using <b>main.py</b> to train the CNN in both the ways described above. Use the following configurations while training:<br>\n",
    "- Use SGD optimizer with learning rate = 0.001, momentum = 0.9 and cross-entropy as the loss function.\n",
    "- Use Adam optimizer with learning rate = 0.01 and cross-entropy as the loss function.\n",
    "<br>\n",
    "You can use early stopping too if loss converges beforehand.<br><br>\n",
    "For each of the two configurations above under each way (described under Model), retain/save the best model (yielding best test set accuracy while testing at each epoch).<br>\n",
    "<span style=\"color:blue\">Use the best saved models to report the final test set accuracies for all the four combinations.</span><br><br>\n",
    "<b>Working:</b><br>Initially, defined all the required optimizers for all the optimizers, and for the two freezed models, I explicitly define the optimizer such that it would only work on the last FC layer's parameters. Then, I just define the train and test function which would hence perform the model operations on each batch of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining all the required characteristics of the model, 1 is for the model, _optim for the respective optimizers\n",
    "optimizer1_SGD = optim.SGD(net1_SGD.parameters(),lr = 0.001,momentum=0.9)\n",
    "optimizer1_Adam = optim.Adam(net1_Adam.parameters(),lr = 0.01)\n",
    "optimizer2_SGD = optim.SGD(net2_SGD.fc.parameters(),lr = 0.001,momentum=0.9)\n",
    "optimizer2_Adam = optim.Adam(net2_Adam.fc.parameters(),lr = 0.01)\n",
    "\n",
    "## defining the required cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# required function for Training the model\n",
    "## inputs -> model and optimizer for that model \n",
    "## outputs -> loss, and accuracy for each epoch for that model\n",
    "\n",
    "def train(model,optimizer):\n",
    "    print('\\nEpoch {}/{}'.format(epoch+1, 200))\n",
    "    print('------------------------------------')\n",
    "    model.train()\n",
    "    batch_loss = 0.0             ## adds loss for each mini-batch for that epoch\n",
    "    correct = 0                  ## stores all the correct predictions for that epoch\n",
    "    \n",
    "    ## moving through batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader): \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        ## zeroing the gradients for that mini batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## outputs from the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        ## evaluate the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        ## backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        ## take the gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## adding the loss for that mini batch\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        ## evaluate the prediction classes for that mini batch\n",
    "        _,pred = torch.max(outputs,dim = 1)\n",
    "        \n",
    "        ## summing the correct predictions \n",
    "        correct += torch.sum(pred == targets).item()\n",
    "        \n",
    "    ## printing the training loss and accuracy for each epoch\n",
    "    print(f'Training Loss: {batch_loss/len(trainloader.dataset):.4f}, Training Accuracy: {(100*correct/len(trainloader.dataset)):.4f}')\n",
    "    loss = batch_loss/len(trainloader.dataset)\n",
    "    accuracy = 100*correct/len(trainloader.dataset)\n",
    "    \n",
    "    ## returning the epoch loss and accuracy\n",
    "    return (loss,accuracy)\n",
    "    \n",
    "## inputs -> \n",
    "## model and optimizer for that model\n",
    "\n",
    "## early stop is just an mutable list, which stores number of epochs till which no better model has occured\n",
    "## and boolean for early-stopping condition,\n",
    "## no_improve is just a param, which indicates till which epoch we would check for no better model\n",
    "\n",
    "## model_name and optim_name are just strings for the required models, which saves the best model in the\n",
    "## current directory until the testing runs\n",
    "\n",
    "## outputs -> loss, accuracy, and some prediction_samples for each epoch for that model\n",
    "\n",
    "# required function for Testing the model\n",
    "def test(model,early_stop,model_name,optim_name,no_improve = 10):\n",
    "    \n",
    "    ## taking the global best_acc to save the best test model \n",
    "    global best_loss\n",
    "    \n",
    "    ## to use the model for testing, such that weight parameters would not get updated\n",
    "    model.eval()\n",
    "    \n",
    "    ## required variables\n",
    "    batch_loss = 0             ## stores the total loss for that epoch\n",
    "    correct_t = 0              ## number of correct predictions for that epoch\n",
    "    prediction_samples = []    ## saves some prediction samples for each mini-batch\n",
    "    predictions = []           ## predictions list for that epoch, to be used for confusion matrix for best model\n",
    "    targetlist = []            ## target list for that epoch, to be used for confusion matrix for best model\n",
    "    \n",
    "    ## use this, so that weights don't get updated\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        ## iterating through batches\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            ## taking outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            ## adding the loss to total loss\n",
    "            batch_loss += criterion(outputs,targets).item()\n",
    "            \n",
    "            ## evaluating the predictions \n",
    "            _,pred_t = torch.max(outputs,dim = 1)\n",
    "            \n",
    "            ## summing the correct predictions \n",
    "            correct_t += torch.sum(pred_t == targets).item()\n",
    "            \n",
    "            ## adding the predictions and targets in their list\n",
    "            predictions.extend(pred_t)\n",
    "            targetlist.extend(targets)\n",
    "            \n",
    "            ## list for sample of predictions samples\n",
    "            prediction_samples.append(wandb.Image(inputs[0],caption = \"Prediction: {} Truth: {}\".format(pred_t[0].item(),targets[0])))\n",
    "        ## batch loss and accuracy\n",
    "        loss = batch_loss/len(testloader.dataset)\n",
    "        curr_acc = 100*correct_t/len(testloader.dataset)\n",
    "        print(f'Testing Loss: {loss:.4f}, Testing Accuracy: {curr_acc:.4f}')\n",
    "        \n",
    "        # Save checkpoint for the model which yields best accuracy\n",
    "        ## checking if current accuracy if larger or not \n",
    "        if loss < best_loss:\n",
    "            \n",
    "            ## since the elements would be tensors, hence taking their values only\n",
    "            predictions = [x.item() for x in predictions]\n",
    "            targetlist = [x.item() for x in targetlist]\n",
    "            \n",
    "            early_stop[1] = 0      ## since got a new model, hence better model count becomes 0\n",
    "            best_loss = loss    ## changing the best accuracy respectively\n",
    "            \n",
    "            ## saving this model with all the required information\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'Testing Loss': loss,\n",
    "            'Testing Accuracy': curr_acc,\n",
    "            'Predictions': predictions,\n",
    "            'Target List': targetlist\n",
    "            },'resnet18_'+ model_name + '_' + optim_name + '.pth')\n",
    "            \n",
    "        else:\n",
    "            early_stop[1] += 1      ## adding the no. of unimproves models\n",
    "            \n",
    "        ## if early stop condition becomes true, then changing the respective boolean true\n",
    "        if early_stop[1] > no_improve:\n",
    "            early_stop[0] = True\n",
    "            \n",
    "        ## returning the loss, accuracy, and predictions\n",
    "        return (loss,curr_acc,prediction_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 : Unfreezed ResNet-18 Model with SDM Optimizer\n",
    "This is ResNet-18 model where all the layers are kept unfreezed so that all that parameters are available for training, and the respective optimizer used is the SDM Optimizer. I have respectively initialized the wandb run, with the respective run name and project name. Also, saved the respective model's configuration parameters.<br>\n",
    "Here, using wandb, I log all the respective values that need to be stored for the run name and project, for each epoch. Also, after saving the model using torch previously, I save the best model as an artifact for that run  using wandb. Finally, I finish that run so that nothing extra would get logged for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0102, Training Accuracy: 54.0980\n",
      "Testing Loss: 0.0094, Testing Accuracy: 67.1700\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0070, Training Accuracy: 68.6060\n",
      "Testing Loss: 0.0080, Testing Accuracy: 72.4300\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0060, Training Accuracy: 72.9060\n",
      "Testing Loss: 0.0070, Testing Accuracy: 76.0900\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0054, Training Accuracy: 75.5920\n",
      "Testing Loss: 0.0065, Testing Accuracy: 77.4500\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0050, Training Accuracy: 77.5980\n",
      "Testing Loss: 0.0063, Testing Accuracy: 77.9200\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0047, Training Accuracy: 78.8060\n",
      "Testing Loss: 0.0060, Testing Accuracy: 79.0800\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0044, Training Accuracy: 79.9700\n",
      "Testing Loss: 0.0059, Testing Accuracy: 79.8800\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0042, Training Accuracy: 80.9700\n",
      "Testing Loss: 0.0057, Testing Accuracy: 80.1800\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0040, Training Accuracy: 82.0920\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.2100\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0038, Training Accuracy: 82.8160\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.1500\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0037, Training Accuracy: 83.5680\n",
      "Testing Loss: 0.0054, Testing Accuracy: 81.8900\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0035, Training Accuracy: 84.2200\n",
      "Testing Loss: 0.0054, Testing Accuracy: 81.9000\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0034, Training Accuracy: 84.8320\n",
      "Testing Loss: 0.0054, Testing Accuracy: 81.9300\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0032, Training Accuracy: 85.2920\n",
      "Testing Loss: 0.0055, Testing Accuracy: 81.9200\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0031, Training Accuracy: 86.0620\n",
      "Testing Loss: 0.0053, Testing Accuracy: 82.2300\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0030, Training Accuracy: 86.6580\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.2400\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0028, Training Accuracy: 87.2820\n",
      "Testing Loss: 0.0053, Testing Accuracy: 82.7700\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0027, Training Accuracy: 87.7240\n",
      "Testing Loss: 0.0053, Testing Accuracy: 82.7200\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0026, Training Accuracy: 87.9580\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.2900\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0025, Training Accuracy: 88.4960\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.7500\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0025, Training Accuracy: 88.8820\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.7900\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0024, Training Accuracy: 89.1680\n",
      "Testing Loss: 0.0053, Testing Accuracy: 83.4000\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0023, Training Accuracy: 89.5500\n",
      "Testing Loss: 0.0054, Testing Accuracy: 83.3500\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0022, Training Accuracy: 89.9900\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.3800\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0021, Training Accuracy: 90.4660\n",
      "Testing Loss: 0.0054, Testing Accuracy: 83.2300\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0020, Training Accuracy: 90.7520\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.4200\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0020, Training Accuracy: 90.8480\n",
      "Testing Loss: 0.0056, Testing Accuracy: 83.2700\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0019, Training Accuracy: 91.1240\n",
      "Testing Loss: 0.0056, Testing Accuracy: 83.3200\n",
      "##################################\n",
      "Run Finished\n",
      "Hence,The best model Accuracy for Resnet-18 Unfreezed with SDM Optimization:  82.77\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "## starting the respective run\n",
    "run = wandb.init(name = \"Unfreezed_SGD\",project = \"18MA20050_Assignment_2\",resume = True)\n",
    "\n",
    "## for watching all the respective information for that model\n",
    "wandb.watch(net1_SGD, log = \"all\")\n",
    "\n",
    "## configuration parameters\n",
    "config = wandb.config\n",
    "config.batch_size = 128\n",
    "config.test_batch_size = 100\n",
    "config.lr = 0.001\n",
    "config.momentum = 0.9\n",
    "\n",
    "## required for early stopping\n",
    "early_stop = [False,0]\n",
    "\n",
    "## using the respective best loss variable \n",
    "global best_loss\n",
    "best_loss = float('inf')\n",
    "\n",
    "## running for large number of epochs, as the model is would be more likely for early stopping\n",
    "for epoch in range(0,200):\n",
    "    \n",
    "    ## runnning the respective functions, and hence getting the outputs\n",
    "    train_loss,train_acc = train(net1_SGD,optimizer1_SGD)\n",
    "    test_loss,test_acc,samples = test(net1_SGD,early_stop,\"Unfreezed\",\"SGD\")\n",
    "    \n",
    "    ## logging each of those results\n",
    "    wandb.log({\n",
    "                \"Training Loss\":train_loss,\n",
    "                \"Training Accuracy\":train_acc,\n",
    "                \"Testing Loss\":test_loss,\n",
    "                \"Testing Accuracy\": test_acc,\n",
    "                \"Prediction Samples\":samples\n",
    "                })\n",
    "    \n",
    "    ## if want to stop early\n",
    "    if early_stop[0] :\n",
    "        break\n",
    "\n",
    "## saving the respective model as an artifact in that run\n",
    "resnet18_Unfreezed_SGD = wandb.Artifact('resnet18_Unfreezed_SGD', type='model')\n",
    "resnet18_Unfreezed_SGD.add_file('resnet18_Unfreezed_SGD.pth')\n",
    "run.log_artifact(resnet18_Unfreezed_SGD)\n",
    "\n",
    "## loading the best model saved, to access the best predictions and accruacy  \n",
    "best_resnet18_Unfreezed_SGD = torch.load('resnet18_Unfreezed_SGD.pth')\n",
    "\n",
    "## logging the confusion matrix for the best model\n",
    "wandb.log({\n",
    "    \"resnet18_Unfreezed_SGD_Confusion_Matrix\": wandb.plot.confusion_matrix(\n",
    "    preds = best_resnet18_Unfreezed_SGD['Predictions'],\n",
    "    y_true = best_resnet18_Unfreezed_SGD['Target List'],\n",
    "    class_names = classes) \n",
    "})\n",
    "\n",
    "## logging the best test accuracy \n",
    "config.best_test_accuracy = best_resnet18_Unfreezed_SGD['Testing Accuracy']\n",
    "\n",
    "## hence finishing the run\n",
    "wandb.finish()\n",
    "print('##################################')\n",
    "print('Run Finished')\n",
    "print('Hence,The best model Accuracy for Resnet-18 Unfreezed with SDM Optimization: ',best_resnet18_Unfreezed_SGD['Testing Accuracy'])\n",
    "print('##################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 : Unfreezed ResNet-18 Model with Adam Optimizer\n",
    "This is ResNet-18 model where all the layers are kept unfreezed so that all that parameters are available for training, and the respective optimizer used is the Adam Optimizer. I have respectively initialized the wandb run, with the respective run name and project name. Also, saved the respective model's configuration parameters.<br>\n",
    "Here, using wandb, I log all the respective values that need to be stored for the run name and project, for each epoch. Also, after saving the model using torch previously, I save the best model as an artifact for that run  using wandb. Finally, I finish that run so that nothing extra would get logged for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0164, Training Accuracy: 25.4280\n",
      "Testing Loss: 0.2203, Testing Accuracy: 34.0100\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0130, Training Accuracy: 38.6140\n",
      "Testing Loss: 0.0149, Testing Accuracy: 47.4200\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0112, Training Accuracy: 46.9600\n",
      "Testing Loss: 0.0133, Testing Accuracy: 51.9400\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0098, Training Accuracy: 54.7420\n",
      "Testing Loss: 0.0146, Testing Accuracy: 50.9500\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0087, Training Accuracy: 59.9920\n",
      "Testing Loss: 0.0101, Testing Accuracy: 64.5500\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0079, Training Accuracy: 64.2440\n",
      "Testing Loss: 0.0109, Testing Accuracy: 63.1200\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0082, Training Accuracy: 62.8400\n",
      "Testing Loss: 0.0102, Testing Accuracy: 64.2000\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0074, Training Accuracy: 66.4300\n",
      "Testing Loss: 0.0099, Testing Accuracy: 66.0800\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0068, Training Accuracy: 69.3540\n",
      "Testing Loss: 0.0090, Testing Accuracy: 68.7600\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0064, Training Accuracy: 71.3780\n",
      "Testing Loss: 0.0080, Testing Accuracy: 71.8600\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0061, Training Accuracy: 72.6780\n",
      "Testing Loss: 0.0076, Testing Accuracy: 73.8400\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0058, Training Accuracy: 74.1500\n",
      "Testing Loss: 0.0078, Testing Accuracy: 73.4600\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0056, Training Accuracy: 75.2680\n",
      "Testing Loss: 0.0076, Testing Accuracy: 74.2900\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0053, Training Accuracy: 76.2140\n",
      "Testing Loss: 0.0068, Testing Accuracy: 76.6200\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0051, Training Accuracy: 77.2320\n",
      "Testing Loss: 0.0068, Testing Accuracy: 76.7800\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0050, Training Accuracy: 77.8680\n",
      "Testing Loss: 0.0066, Testing Accuracy: 77.5500\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0048, Training Accuracy: 78.4560\n",
      "Testing Loss: 0.0070, Testing Accuracy: 76.0600\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0047, Training Accuracy: 79.4560\n",
      "Testing Loss: 0.0060, Testing Accuracy: 79.4600\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0046, Training Accuracy: 79.7280\n",
      "Testing Loss: 0.0067, Testing Accuracy: 77.6300\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0045, Training Accuracy: 80.3540\n",
      "Testing Loss: 0.0065, Testing Accuracy: 78.4400\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0043, Training Accuracy: 80.6380\n",
      "Testing Loss: 0.0063, Testing Accuracy: 78.8500\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0042, Training Accuracy: 81.2820\n",
      "Testing Loss: 0.0065, Testing Accuracy: 78.0200\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0041, Training Accuracy: 81.6040\n",
      "Testing Loss: 0.0061, Testing Accuracy: 79.7500\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0040, Training Accuracy: 82.2380\n",
      "Testing Loss: 0.0059, Testing Accuracy: 79.9000\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0039, Training Accuracy: 82.6320\n",
      "Testing Loss: 0.0057, Testing Accuracy: 80.5500\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0039, Training Accuracy: 82.8880\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.2000\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0038, Training Accuracy: 83.2420\n",
      "Testing Loss: 0.0060, Testing Accuracy: 79.5600\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0037, Training Accuracy: 83.6900\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.2100\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0036, Training Accuracy: 83.8720\n",
      "Testing Loss: 0.0055, Testing Accuracy: 81.4700\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0036, Training Accuracy: 84.0840\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.3100\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0035, Training Accuracy: 84.7200\n",
      "Testing Loss: 0.0057, Testing Accuracy: 80.9800\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0034, Training Accuracy: 84.7620\n",
      "Testing Loss: 0.0057, Testing Accuracy: 81.3700\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0034, Training Accuracy: 85.0300\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.9500\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0036, Training Accuracy: 84.0360\n",
      "Testing Loss: 0.0055, Testing Accuracy: 82.2300\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0033, Training Accuracy: 85.2940\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.2000\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0039, Training Accuracy: 83.0320\n",
      "Testing Loss: 0.0054, Testing Accuracy: 81.8200\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0032, Training Accuracy: 85.7000\n",
      "Testing Loss: 0.0053, Testing Accuracy: 82.4200\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0031, Training Accuracy: 86.0460\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.5000\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0030, Training Accuracy: 86.7880\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.0200\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0030, Training Accuracy: 86.8760\n",
      "Testing Loss: 0.0056, Testing Accuracy: 81.6600\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0029, Training Accuracy: 86.9880\n",
      "Testing Loss: 0.0052, Testing Accuracy: 83.4600\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0029, Training Accuracy: 87.1460\n",
      "Testing Loss: 0.0053, Testing Accuracy: 83.2800\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0029, Training Accuracy: 87.2960\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.6100\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0028, Training Accuracy: 87.4240\n",
      "Testing Loss: 0.0052, Testing Accuracy: 82.9000\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0028, Training Accuracy: 87.6320\n",
      "Testing Loss: 0.0053, Testing Accuracy: 83.4000\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0028, Training Accuracy: 87.5920\n",
      "Testing Loss: 0.0053, Testing Accuracy: 82.7500\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0027, Training Accuracy: 87.7260\n",
      "Testing Loss: 0.0054, Testing Accuracy: 82.9500\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0027, Training Accuracy: 87.9220\n",
      "Testing Loss: 0.0055, Testing Accuracy: 82.7100\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0027, Training Accuracy: 88.1320\n",
      "Testing Loss: 0.0053, Testing Accuracy: 83.5100\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0026, Training Accuracy: 88.3240\n",
      "Testing Loss: 0.0053, Testing Accuracy: 83.1200\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0026, Training Accuracy: 88.5740\n",
      "Testing Loss: 0.0051, Testing Accuracy: 83.6900\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0025, Training Accuracy: 88.8040\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.3300\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0025, Training Accuracy: 88.8460\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.1700\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0025, Training Accuracy: 88.8180\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.0300\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0024, Training Accuracy: 89.1020\n",
      "Testing Loss: 0.0057, Testing Accuracy: 82.4500\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0025, Training Accuracy: 89.0520\n",
      "Testing Loss: 0.0057, Testing Accuracy: 82.0300\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0024, Training Accuracy: 89.3920\n",
      "Testing Loss: 0.0052, Testing Accuracy: 84.2700\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0024, Training Accuracy: 89.1460\n",
      "Testing Loss: 0.0055, Testing Accuracy: 83.7000\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0023, Training Accuracy: 89.4620\n",
      "Testing Loss: 0.0053, Testing Accuracy: 84.2100\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0023, Training Accuracy: 89.7500\n",
      "Testing Loss: 0.0054, Testing Accuracy: 83.8200\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0023, Training Accuracy: 89.7820\n",
      "Testing Loss: 0.0053, Testing Accuracy: 84.1100\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0022, Training Accuracy: 90.0360\n",
      "Testing Loss: 0.0056, Testing Accuracy: 83.5900\n",
      "##################################\n",
      "Run Finished\n",
      "Hence,The best model Accuracy for Resnet-18 Unfreezed with Adam Optimization:  83.69\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "## starting the respective run\n",
    "run = wandb.init(name = \"Unfreezed_Adam\",project = \"18MA20050_Assignment_2\")\n",
    "\n",
    "## for watching all the respective information for that model\n",
    "wandb.watch(net1_Adam, log = \"all\")\n",
    "\n",
    "## configuration parameters\n",
    "config = wandb.config\n",
    "config.batch_size = 128\n",
    "config.test_batch_size = 100\n",
    "config.lr = 0.01\n",
    "\n",
    "## required for early stopping\n",
    "early_stop = [False,0]\n",
    "\n",
    "## using the respective best loss variable \n",
    "global best_loss\n",
    "best_loss = float('inf')\n",
    "\n",
    "## running for large number of epochs, as the model is would be more likely for early stopping\n",
    "for epoch in range(0,200):\n",
    "    \n",
    "    ## runnning the respective functions, and hence getting the outputs\n",
    "    train_loss,train_acc = train(net1_Adam,optimizer1_Adam)\n",
    "    test_loss,test_acc,samples = test(net1_Adam,early_stop,\"Unfreezed\",\"Adam\")\n",
    "    \n",
    "    ## logging each of those results\n",
    "    wandb.log({\n",
    "                \"Training Loss\":train_loss,\n",
    "                \"Training Accuracy\":train_acc,\n",
    "                \"Testing Loss\":test_loss,\n",
    "                \"Testing Accuracy\": test_acc,\n",
    "                \"Prediction Samples\":samples\n",
    "                })\n",
    "    ## for ealy stopping\n",
    "    if early_stop[0] :\n",
    "        break\n",
    "    \n",
    "## saving the respective model as an artifact in that run\n",
    "resnet18_Unfreezed_Adam = wandb.Artifact('resnet18_Unfreezed_Adam', type='model')\n",
    "resnet18_Unfreezed_Adam.add_file('resnet18_Unfreezed_Adam.pth')\n",
    "run.log_artifact(resnet18_Unfreezed_Adam)\n",
    "\n",
    "## loading the best model saved, to access the best predictions and accruacy  \n",
    "F# logging the confusion matrix for the best model\n",
    "wandb.log({\n",
    "    \"resnet18_Unfreezed_Adam_Confusion_Matrix\": wandb.plot.confusion_matrix(\n",
    "    preds = best_resnet18_Unfreezed_Adam['Predictions'],\n",
    "    y_true = best_resnet18_Unfreezed_Adam['Target List'],\n",
    "    class_names = classes) \n",
    "})\n",
    "\n",
    "## logging the best test accuracy \n",
    "config.best_test_accuracy = best_resnet18_Unfreezed_Adam['Testing Accuracy']\n",
    "\n",
    "## hence finishing the run\n",
    "wandb.finish()\n",
    "print('##################################')\n",
    "print('Run Finished')\n",
    "print('Hence,The best model Accuracy for Resnet-18 Unfreezed with Adam Optimization: ',best_resnet18_Unfreezed_Adam['Testing Accuracy'])\n",
    "print('##################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model3 : Freezed ResNet-18 Model with SDM Optimizer\n",
    "This is ResNet-18 model where all the layers except the last layer(FC layer) are kept Freezed, so that only the parameters for the last layer is available for training, and the respective optimizer used is the SDM Optimizer. I have respectively initialized the wandb run, with the respective run name and project name. Also, saved the respective model's configuration parameters.<br>\n",
    "Here, using wandb, I log all the respective values that need to be stored for the run name and project, for each epoch. Also, after saving the model using torch previously, I save the best model as an artifact for that run  using wandb. Finally, I finish that run so that nothing extra would get logged for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0153, Training Accuracy: 31.1780\n",
      "Testing Loss: 0.0180, Testing Accuracy: 37.3200\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0137, Training Accuracy: 38.7100\n",
      "Testing Loss: 0.0176, Testing Accuracy: 38.9400\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0134, Training Accuracy: 40.3580\n",
      "Testing Loss: 0.0173, Testing Accuracy: 40.1400\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0132, Training Accuracy: 40.9620\n",
      "Testing Loss: 0.0171, Testing Accuracy: 41.0600\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0131, Training Accuracy: 41.5400\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.6900\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0131, Training Accuracy: 41.3500\n",
      "Testing Loss: 0.0170, Testing Accuracy: 40.8600\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0130, Training Accuracy: 41.7880\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.2300\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0130, Training Accuracy: 41.9860\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.8200\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0130, Training Accuracy: 42.0980\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.7500\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.2240\n",
      "Testing Loss: 0.0168, Testing Accuracy: 42.0200\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.1240\n",
      "Testing Loss: 0.0170, Testing Accuracy: 41.3500\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.2340\n",
      "Testing Loss: 0.0170, Testing Accuracy: 41.3200\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.1060\n",
      "Testing Loss: 0.0170, Testing Accuracy: 41.3800\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.5060\n",
      "Testing Loss: 0.0168, Testing Accuracy: 42.2100\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.2260\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.3500\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.6660\n",
      "Testing Loss: 0.0167, Testing Accuracy: 42.5200\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.4900\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.8000\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.3740\n",
      "Testing Loss: 0.0169, Testing Accuracy: 42.4000\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.4460\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.9900\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.3800\n",
      "Testing Loss: 0.0167, Testing Accuracy: 42.4500\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.5520\n",
      "Testing Loss: 0.0170, Testing Accuracy: 41.3000\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.4520\n",
      "Testing Loss: 0.0168, Testing Accuracy: 41.8700\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.5040\n",
      "Testing Loss: 0.0168, Testing Accuracy: 42.0100\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.4280\n",
      "Testing Loss: 0.0168, Testing Accuracy: 42.3800\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.5880\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.8700\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.8560\n",
      "Testing Loss: 0.0169, Testing Accuracy: 41.6900\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.5720\n",
      "Testing Loss: 0.0167, Testing Accuracy: 42.2700\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.7260\n",
      "Testing Loss: 0.0169, Testing Accuracy: 42.0200\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0129, Training Accuracy: 42.4480\n",
      "Testing Loss: 0.0168, Testing Accuracy: 41.8600\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.6880\n",
      "Testing Loss: 0.0168, Testing Accuracy: 41.6600\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0128, Training Accuracy: 42.5840\n",
      "Testing Loss: 0.0168, Testing Accuracy: 41.8700\n",
      "##################################\n",
      "Run Finished\n",
      "Hence,The best model Accuracy for Resnet-18 Freezed with SDM Optimization:  42.45\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "## starting the respective run\n",
    "run = wandb.init(name = \"Freezed_SGD\",project = \"18MA20050_Assignment_2\")\n",
    "\n",
    "## for watching all the respective information for that model\n",
    "wandb.watch(net2_SGD, log = \"all\")\n",
    "\n",
    "## configuration parameters\n",
    "config = wandb.config\n",
    "config.batch_size = 128\n",
    "config.test_batch_size = 100\n",
    "config.lr = 0.001\n",
    "config.momentum = 0.9\n",
    "\n",
    "## required for early stopping\n",
    "early_stop = [False,0]\n",
    "\n",
    "## using the respective best loss variable \n",
    "global best_loss\n",
    "best_loss = float('inf')\n",
    "\n",
    "## running for large number of epochs, as the model is would be more likely for early stopping\n",
    "for epoch in range(0,200):\n",
    "    \n",
    "    ## runnning the respective functions, and hence getting the outputs\n",
    "    train_loss,train_acc = train(net2_SGD,optimizer2_SGD)\n",
    "    test_loss,test_acc,samples = test(net2_SGD,early_stop,\"Freezed\",\"SGD\")\n",
    "    \n",
    "    ## logging each of those results\n",
    "    wandb.log({\n",
    "                \"Training Loss\":train_loss,\n",
    "                \"Training Accuracy\":train_acc,\n",
    "                \"Testing Loss\":test_loss,\n",
    "                \"Testing Accuracy\": test_acc,\n",
    "                \"Prediction Samples\":samples\n",
    "                })\n",
    "    \n",
    "    ## for ealy stopping\n",
    "    if early_stop[0] :\n",
    "        break\n",
    "        \n",
    "## saving the respective model as an artifact in that run\n",
    "resnet18_Freezed_SGD = wandb.Artifact('resnet18_Freezed_SGD', type='model')\n",
    "resnet18_Freezed_SGD.add_file('resnet18_Freezed_SGD.pth')\n",
    "run.log_artifact(resnet18_Freezed_SGD)\n",
    "\n",
    "## loading the best model saved, to access the best predictions and accruacy\n",
    "best_resnet18_Freezed_SGD = torch.load('resnet18_Freezed_SGD.pth')\n",
    "\n",
    "## logging the confusion matrix for the best model\n",
    "wandb.log({\n",
    "    \"resnet18_Freezed_SGD_Confusion_Matrix\": wandb.plot.confusion_matrix(\n",
    "    preds = best_resnet18_Freezed_SGD['Predictions'],\n",
    "    y_true = best_resnet18_Freezed_SGD['Target List'],\n",
    "    class_names = classes) \n",
    "})\n",
    "\n",
    "## logging the best test accuracy\n",
    "config.best_test_accuracy = best_resnet18_Freezed_SGD['Testing Accuracy']\n",
    "\n",
    "## hence finishing the run\n",
    "wandb.finish()\n",
    "print('##################################')\n",
    "print('Run Finished')\n",
    "print('Hence,The best model Accuracy for Resnet-18 Freezed with SDM Optimization: ',best_resnet18_Freezed_SGD['Testing Accuracy'])\n",
    "print('##################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model4 : Freezed ResNet-18 Model with Adam Optimizer\n",
    "This is ResNet-18 model where all the layers except the last layer(FC layer) are kept Freezed, so that only the parameters for the last layer is available for training, and the respective optimizer used is the Adam Optimizer. I have respectively initialized the wandb run, with the respective run name and project name. Also, saved the respective model's configuration parameters.<br>\n",
    "Here, using wandb, I log all the respective values that need to be stored for the run name and project, for each epoch. Also, after saving the model using torch previously, I save the best model as an artifact for that run  using wandb. Finally, I finish that run so that nothing extra would get logged for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0162, Training Accuracy: 33.7540\n",
      "Testing Loss: 0.0212, Testing Accuracy: 34.0200\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.0900\n",
      "Testing Loss: 0.0206, Testing Accuracy: 36.6600\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0158, Training Accuracy: 35.3820\n",
      "Testing Loss: 0.0220, Testing Accuracy: 34.0100\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.2860\n",
      "Testing Loss: 0.0234, Testing Accuracy: 33.3400\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.1920\n",
      "Testing Loss: 0.0209, Testing Accuracy: 36.6100\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.1420\n",
      "Testing Loss: 0.0221, Testing Accuracy: 34.1100\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.2960\n",
      "Testing Loss: 0.0210, Testing Accuracy: 35.8100\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.1940\n",
      "Testing Loss: 0.0214, Testing Accuracy: 35.3700\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.3340\n",
      "Testing Loss: 0.0246, Testing Accuracy: 33.3700\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.3160\n",
      "Testing Loss: 0.0223, Testing Accuracy: 34.9900\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.5440\n",
      "Testing Loss: 0.0212, Testing Accuracy: 35.2600\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.5120\n",
      "Testing Loss: 0.0212, Testing Accuracy: 36.0600\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.1480\n",
      "Testing Loss: 0.0206, Testing Accuracy: 35.7800\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.0380\n",
      "Testing Loss: 0.0214, Testing Accuracy: 35.5600\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.4980\n",
      "Testing Loss: 0.0221, Testing Accuracy: 36.5100\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0158, Training Accuracy: 35.7680\n",
      "Testing Loss: 0.0221, Testing Accuracy: 33.1900\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.7240\n",
      "Testing Loss: 0.0237, Testing Accuracy: 32.2300\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.7360\n",
      "Testing Loss: 0.0207, Testing Accuracy: 37.4100\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0162, Training Accuracy: 35.2060\n",
      "Testing Loss: 0.0231, Testing Accuracy: 33.4200\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0158, Training Accuracy: 35.3200\n",
      "Testing Loss: 0.0210, Testing Accuracy: 35.7400\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0161, Training Accuracy: 35.1380\n",
      "Testing Loss: 0.0216, Testing Accuracy: 34.6000\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0160, Training Accuracy: 35.5460\n",
      "Testing Loss: 0.0233, Testing Accuracy: 33.3500\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0159, Training Accuracy: 35.6800\n",
      "Testing Loss: 0.0227, Testing Accuracy: 33.5000\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------\n",
      "Training Loss: 0.0158, Training Accuracy: 35.5620\n",
      "Testing Loss: 0.0232, Testing Accuracy: 32.5300\n",
      "##################################\n",
      "Run Finished\n",
      "Hence,The best model Accuracy for Resnet-18 Freezed with Adam Optimization:  35.78\n",
      "##################################\n"
     ]
    }
   ],
   "source": [
    "## starting the respective run\n",
    "run = wandb.init(name = \"Freezed_Adam\",project = \"18MA20050_Assignment_2\")\n",
    "\n",
    "## for watching all the respective information for that model\n",
    "wandb.watch(net2_Adam, log = \"all\")\n",
    "\n",
    "## configuration parameters\n",
    "config = wandb.config\n",
    "config.batch_size = 128\n",
    "config.test_batch_size = 100\n",
    "config.lr = 0.01\n",
    "\n",
    "## required for early stopping\n",
    "early_stop = [False,0]\n",
    "\n",
    "## using the respective best loss variable \n",
    "global best_loss\n",
    "best_loss = float('inf')\n",
    "\n",
    "## running for large number of epochs, as the model is would be more likely for early stopping\n",
    "for epoch in range(0,200):\n",
    "    \n",
    "    ## runnning the respective functions, and hence getting the outputs\n",
    "    train_loss,train_acc = train(net2_Adam,optimizer2_Adam)\n",
    "    test_loss,test_acc,samples = test(net2_Adam,early_stop,\"Freezed\",\"Adam\")\n",
    "    \n",
    "    ## logging each of those results\n",
    "    wandb.log({\n",
    "                \"Training Loss\":train_loss,\n",
    "                \"Training Accuracy\":train_acc,\n",
    "                \"Testing Loss\":test_loss,\n",
    "                \"Testing Accuracy\": test_acc,\n",
    "                \"Prediction Samples\":samples\n",
    "                })\n",
    "    \n",
    "    ## for ealy stopping\n",
    "    if early_stop[0] :\n",
    "        break\n",
    "        \n",
    "## saving the respective model as an artifact in that run\n",
    "resnet18_Freezed_Adam = wandb.Artifact('resnet18_Freezed_Adam', type='model')\n",
    "resnet18_Freezed_Adam.add_file('resnet18_Freezed_Adam.pth')\n",
    "run.log_artifact(resnet18_Freezed_Adam)\n",
    "\n",
    "## loading the best model saved, to access the best predictions and accruacy\n",
    "best_resnet18_Freezed_Adam = torch.load('resnet18_Freezed_Adam.pth')\n",
    "\n",
    "## logging the confusion matrix for the best model\n",
    "wandb.log({\n",
    "    \"resnet18_Freezed_SGD_Confusion_Matrix\": wandb.plot.confusion_matrix(\n",
    "    preds = best_resnet18_Freezed_Adam['Predictions'],\n",
    "    y_true = best_resnet18_Freezed_Adam['Target List'],\n",
    "    class_names = classes) \n",
    "})\n",
    "\n",
    "## logging the best test accuracy\n",
    "config.best_test_accuracy = best_resnet18_Freezed_Adam['Testing Accuracy']\n",
    "\n",
    "## hence finishing the run\n",
    "wandb.finish()\n",
    "print('##################################')\n",
    "print('Run Finished')\n",
    "print('Hence,The best model Accuracy for Resnet-18 Freezed with Adam Optimization: ',best_resnet18_Freezed_Adam['Testing Accuracy'])\n",
    "print('##################################')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
